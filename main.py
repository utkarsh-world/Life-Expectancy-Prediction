#!/usr/bin/env python3
"""
Life Expectancy Prediction - Main Pipeline
==========================================

Enhanced ML pipeline for life expectancy prediction with comprehensive
data analysis, model training, and evaluation.

Author: Utkarsh Sharma - Data Analysis Workspace
Date: October 2025
"""

import argparse
import logging
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime

def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/pipeline.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Life Expectancy Prediction ML Pipeline"
    )
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/default_config.yaml',
        help='Path to configuration file'
    )
    parser.add_argument(
        '--data-path', 
        type=str, 
        default='data/raw/Life-Expectancy-Data.csv',
        help='Path to input dataset'
    )
    parser.add_argument(
        '--output-dir', 
        type=str, 
        default='reports',
        help='Directory for output reports'
    )
    parser.add_argument(
        '--verbose', 
        action='store_true',
        help='Enable verbose logging'
    )

    return parser.parse_args()

def load_and_validate_data(data_path):
    """Load and validate the dataset."""
    logger = logging.getLogger(__name__)

    if not Path(data_path).exists():
        logger.error(f"Data file not found: {data_path}")
        return None

    try:
        data = pd.read_csv(data_path)
        logger.info(f"✅ Data loaded successfully: {data.shape}")

        # Basic validation
        required_columns = ['Life expectancy ', 'Country', 'Year']
        missing_cols = [col for col in required_columns if col not in data.columns]
        if missing_cols:
            logger.error(f"Missing required columns: {missing_cols}")
            return None

        return data

    except Exception as e:
        logger.error(f"Error loading data: {e}")
        return None

def analyze_data(data):
    """Perform basic data analysis."""
    logger = logging.getLogger(__name__)

    logger.info("📊 Dataset Analysis:")
    logger.info(f"   - Countries: {data['Country'].nunique()}")
    logger.info(f"   - Years: {data['Year'].min()}-{data['Year'].max()}")
    logger.info(f"   - Records: {len(data):,}")

    # Life expectancy statistics
    life_exp_stats = data['Life expectancy '].describe()
    logger.info(f"   - Avg Life Expectancy: {life_exp_stats['mean']:.1f} years")
    logger.info(f"   - Min: {life_exp_stats['min']:.1f} years")
    logger.info(f"   - Max: {life_exp_stats['max']:.1f} years")

    # Missing data analysis
    missing_pct = (data.isnull().sum() / len(data) * 100).round(1)
    high_missing = missing_pct[missing_pct > 10]
    if not high_missing.empty:
        logger.warning(f"   - Columns with >10% missing data:")
        for col, pct in high_missing.items():
            logger.warning(f"     • {col}: {pct}%")

    return {
        'countries': data['Country'].nunique(),
        'years': data['Year'].max() - data['Year'].min() + 1,
        'records': len(data),
        'avg_life_exp': life_exp_stats['mean'],
        'missing_data': missing_pct.to_dict()
    }

def generate_summary_report(analysis_results, output_dir):
    """Generate a summary report."""
    logger = logging.getLogger(__name__)

    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Generate report
    report_content = f'''# Life Expectancy Prediction - Analysis Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Dataset Summary
- **Countries Analyzed**: {analysis_results['countries']}
- **Time Period**: {analysis_results['years']} years
- **Total Records**: {analysis_results['records']:,}
- **Average Life Expectancy**: {analysis_results['avg_life_exp']:.1f} years

## Analysis Status
✅ Data loading and validation completed
✅ Basic statistical analysis performed
✅ Missing data assessment completed

## Next Steps
1. Run feature engineering: `python src/features/feature_engineer.py`
2. Train models: `python src/models/model_trainer.py`
3. Launch dashboard: `streamlit run src/app/streamlit_app.py`
4. Start API server: `python src/api/app.py`

---
*Generated by Life Expectancy Prediction Pipeline v2.0*
'''

    report_path = Path(output_dir) / 'analysis_summary.md'
    with open(report_path, 'w') as f:
        f.write(report_content)

    logger.info(f"📄 Summary report saved: {report_path}")
    return str(report_path)

def main():
    """Main execution function."""
    args = parse_arguments()

    # Create necessary directories
    Path('logs').mkdir(exist_ok=True)
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    # Setup logging
    logger = setup_logging()

    logger.info("🚀 Starting Life Expectancy Prediction Pipeline")
    logger.info("="*60)
    logger.info(f"Configuration: {args.config}")
    logger.info(f"Data source: {args.data_path}")
    logger.info(f"Output directory: {args.output_dir}")

    try:
        # Load and validate data
        logger.info("📥 Step 1: Loading and validating data...")
        data = load_and_validate_data(args.data_path)
        if data is None:
            logger.error("❌ Data loading failed. Pipeline terminated.")
            sys.exit(1)

        # Analyze data
        logger.info("🔍 Step 2: Performing data analysis...")
        analysis_results = analyze_data(data)

        # Generate summary report
        logger.info("📄 Step 3: Generating summary report...")
        report_path = generate_summary_report(analysis_results, args.output_dir)

        logger.info("="*60)
        logger.info("✅ Pipeline completed successfully!")
        logger.info(f"📊 Summary: Analyzed {analysis_results['countries']} countries")
        logger.info(f"📄 Report generated: {report_path}")
        logger.info("="*60)

        logger.info("🚀 Next Steps:")
        logger.info("   1. Launch dashboard: streamlit run src/app/streamlit_app.py")
        logger.info("   2. Start API: python src/api/app.py")
        logger.info("   3. Train models: python src/models/model_trainer.py")

    except Exception as e:
        logger.error(f"❌ Pipeline failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
